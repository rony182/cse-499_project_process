{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "# Initialize Pinecone client with your API key\n",
    "pc = Pinecone(api_key='14cfeeab-10fd-46dd-a514-0b3195137596')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"investments\"  # Set this to the name of your index\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # Set this to 384 for your embeddings\n",
    "        metric=\"cosine\",  # You can also choose \"euclidean\" or \"dotproduct\"\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'  # You can choose a different region if needed\n",
    "        ) \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "def estimate_record_size(record):\n",
    "    id_size = len(record['id'].encode('utf-8'))  # Size of the ID in bytes\n",
    "    embedding_size = len(record['embedding']) * 4  # 4 bytes per float32 value\n",
    "    metadata_size = len(json.dumps(record['metadata']).encode('utf-8'))  # Size of metadata as a JSON string in bytes\n",
    "    total_size = id_size + embedding_size + metadata_size\n",
    "    return total_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BATCH_SIZE = 2 * 1024 * 1024  # 2 MB in bytes\n",
    "\n",
    "def create_batches(data):\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_batch_size = 0\n",
    "\n",
    "    for record in data:\n",
    "        record_size = estimate_record_size(record)\n",
    "        \n",
    "        # If adding this record would exceed the 2 MB limit, finalize the current batch and start a new one\n",
    "        if current_batch_size + record_size > MAX_BATCH_SIZE:\n",
    "            batches.append(current_batch)\n",
    "            current_batch = [record]\n",
    "            current_batch_size = record_size\n",
    "        else:\n",
    "            current_batch.append(record)\n",
    "            current_batch_size += record_size\n",
    "\n",
    "    # Don't forget to add the last batch\n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "    \n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data loaded from combined_data.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load the combined_data from the file\n",
    "filename = 'combined_data.pkl'\n",
    "\n",
    "with open(filename, 'rb') as file:\n",
    "    combined_data = pickle.load(file)\n",
    "\n",
    "print(f\"Combined data loaded from {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def split_large_metadata(record, max_size=40000):\n",
    "    metadata_str = json.dumps(record['metadata'])\n",
    "    \n",
    "    # Check if the metadata size exceeds the maximum allowed size\n",
    "    if len(metadata_str.encode('utf-8')) > max_size:\n",
    "        # Split the narrative_texts into smaller parts\n",
    "        if 'narrative_texts' in record['metadata']:\n",
    "            narratives = record['metadata']['narrative_texts']\n",
    "            parts = []\n",
    "            while narratives:\n",
    "                part = narratives[:max_size]\n",
    "                narratives = narratives[max_size:]\n",
    "                parts.append(part)\n",
    "            \n",
    "            # Create new records for each part\n",
    "            new_records = []\n",
    "            for i, part in enumerate(parts):\n",
    "                new_metadata = record['metadata'].copy()\n",
    "                new_metadata['narrative_texts'] = part\n",
    "                new_records.append({\n",
    "                    'id': f\"{record['id']}_part_{i}\",\n",
    "                    'embedding': record['embedding'],\n",
    "                    'metadata': new_metadata,\n",
    "                    'text': record['text']\n",
    "                })\n",
    "            return new_records\n",
    "    \n",
    "    # Return the original record if no splitting is needed\n",
    "    return [record]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split into 54371 records.\n"
     ]
    }
   ],
   "source": [
    "# Create a new list to store the split records\n",
    "new_combined_data = []\n",
    "\n",
    "# Apply the splitting function to each record\n",
    "for record in combined_data:\n",
    "    new_combined_data.extend(split_large_metadata(record))\n",
    "\n",
    "# Replace the original combined_data with the new, split version\n",
    "combined_data = new_combined_data\n",
    "\n",
    "print(f\"Data has been split into {len(combined_data)} records.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that no record exceeds the metadata size limit\n",
    "for i, record in enumerate(combined_data):\n",
    "    metadata_str = json.dumps(record['metadata'])\n",
    "    if len(metadata_str.encode('utf-8')) > 40960:\n",
    "        print(f\"Record {i} exceeds the size limit with ID: {record['id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BATCH_SIZE = 2 * 1024 * 1024  # 2 MB in bytes\n",
    "MAX_VECTORS_PER_BATCH = 1000  # Pinecone's limit\n",
    "\n",
    "def create_batches(data):\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_batch_size = 0\n",
    "    current_batch_count = 0\n",
    "\n",
    "    for record in data:\n",
    "        record_size = estimate_record_size(record)\n",
    "        \n",
    "        # If adding this record would exceed the 2 MB limit or the vector count limit, finalize the current batch and start a new one\n",
    "        if (current_batch_size + record_size > MAX_BATCH_SIZE) or (current_batch_count >= MAX_VECTORS_PER_BATCH):\n",
    "            batches.append(current_batch)\n",
    "            current_batch = [record]\n",
    "            current_batch_size = record_size\n",
    "            current_batch_count = 1\n",
    "        else:\n",
    "            current_batch.append(record)\n",
    "            current_batch_size += record_size\n",
    "            current_batch_count += 1\n",
    "\n",
    "    # Don't forget to add the last batch\n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "    \n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/55 uploaded successfully!\n",
      "Batch 2/55 uploaded successfully!\n",
      "Batch 3/55 uploaded successfully!\n",
      "Batch 4/55 uploaded successfully!\n",
      "Batch 5/55 uploaded successfully!\n",
      "Batch 6/55 uploaded successfully!\n",
      "Batch 7/55 uploaded successfully!\n",
      "Batch 8/55 uploaded successfully!\n",
      "Batch 9/55 uploaded successfully!\n",
      "Batch 10/55 uploaded successfully!\n",
      "Batch 11/55 uploaded successfully!\n",
      "Batch 12/55 uploaded successfully!\n",
      "Batch 13/55 uploaded successfully!\n",
      "Batch 14/55 uploaded successfully!\n",
      "Batch 15/55 uploaded successfully!\n",
      "Batch 16/55 uploaded successfully!\n",
      "Batch 17/55 uploaded successfully!\n",
      "Batch 18/55 uploaded successfully!\n",
      "Batch 19/55 uploaded successfully!\n",
      "Batch 20/55 uploaded successfully!\n",
      "Batch 21/55 uploaded successfully!\n",
      "Batch 22/55 uploaded successfully!\n",
      "Batch 23/55 uploaded successfully!\n",
      "Batch 24/55 uploaded successfully!\n",
      "Batch 25/55 uploaded successfully!\n",
      "Batch 26/55 uploaded successfully!\n",
      "Batch 27/55 uploaded successfully!\n",
      "Batch 28/55 uploaded successfully!\n",
      "Batch 29/55 uploaded successfully!\n",
      "Batch 30/55 uploaded successfully!\n",
      "Batch 31/55 uploaded successfully!\n",
      "Batch 32/55 uploaded successfully!\n",
      "Batch 33/55 uploaded successfully!\n",
      "Batch 34/55 uploaded successfully!\n",
      "Batch 35/55 uploaded successfully!\n",
      "Batch 36/55 uploaded successfully!\n",
      "Batch 37/55 uploaded successfully!\n",
      "Batch 38/55 uploaded successfully!\n",
      "Batch 39/55 uploaded successfully!\n",
      "Batch 40/55 uploaded successfully!\n",
      "Batch 41/55 uploaded successfully!\n",
      "Batch 42/55 uploaded successfully!\n",
      "Batch 43/55 uploaded successfully!\n",
      "Batch 44/55 uploaded successfully!\n",
      "Batch 45/55 uploaded successfully!\n",
      "Batch 46/55 uploaded successfully!\n",
      "Batch 47/55 uploaded successfully!\n",
      "Batch 48/55 uploaded successfully!\n",
      "Batch 49/55 uploaded successfully!\n",
      "Batch 50/55 uploaded successfully!\n",
      "Batch 51/55 uploaded successfully!\n",
      "Batch 52/55 uploaded successfully!\n",
      "Batch 53/55 uploaded successfully!\n",
      "Batch 54/55 uploaded successfully!\n",
      "Batch 55/55 uploaded successfully!\n",
      "All data has been uploaded to Pinecone successfully!\n"
     ]
    }
   ],
   "source": [
    "# Connect to the Pinecone index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Create and upload batches\n",
    "batches = create_batches(combined_data)\n",
    "\n",
    "for i, batch in enumerate(batches):\n",
    "    vectors = [\n",
    "        {\"id\": record['id'], \"values\": record['embedding'], \"metadata\": record['metadata']}\n",
    "        for record in batch\n",
    "    ]\n",
    "    index.upsert(vectors=vectors)\n",
    "    print(f\"Batch {i+1}/{len(batches)} uploaded successfully!\")\n",
    "\n",
    "print(\"All data has been uploaded to Pinecone successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "investment_chatbot",
   "language": "python",
   "name": "investment_chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
